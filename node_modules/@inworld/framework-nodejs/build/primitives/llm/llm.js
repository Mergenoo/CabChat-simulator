"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLM = void 0;
const common_1 = require("../../common");
const helpers_1 = require("../../common/helpers");
const stream_1 = require("../../common/stream");
const expose_binary_1 = require("../../expose_binary");
const messages_1 = require("./messages");
const text_generator_1 = require("./text_generator");
/**
 * Class for interacting with Language Models (LLMs).
 * Provides functionality to generate text using prompts or message sequences.
 */
class LLM {
    /**
     * Creates a new LLM instance.
     *
     * @param llm - External reference to the native LLM implementation
     * @param factory - External reference to the LLM factory
     * @internal
     */
    constructor(llm, factory) {
        this.llm = llm;
        this.factory = factory;
    }
    /**
     * Generates text based on a prompt or message sequence.
     *
     * @param props - Configuration for text generation
     * @param props.prompt - Optional text prompt to use for generation
     * @param props.messages - Optional array of message objects for chat-like interactions
     * @param props.config - Configuration parameters for text generation
     * @returns A stream iterator for the generated text
     * @throws InworldError if neither prompt nor messages are provided, or generation fails
     */
    generateText(props) {
        return __awaiter(this, void 0, void 0, function* () {
            if (!props.prompt && !props.messages) {
                throw new common_1.InworldError('Either prompt or messages must be provided for LLM generation');
            }
            let generateStatus;
            let messages = null;
            try {
                messages = new messages_1.Messages(props.messages);
                const generateStatus = yield this.buildGenerateStatus({
                    prompt: props.prompt,
                    messages: messages,
                    config: props.config,
                });
                if (!expose_binary_1.LLMInterfaceFunctions.isOK(generateStatus)) {
                    throw new common_1.InworldError('Failed to generate text', (0, helpers_1.getStatus)(generateStatus));
                }
                return this.streamIterator(expose_binary_1.InputStreamFunctions.get(generateStatus), messages);
            }
            catch (error) {
                messages === null || messages === void 0 ? void 0 : messages.destroy();
                throw error;
            }
            finally {
                if (generateStatus) {
                    expose_binary_1.InputStreamFunctions.delete(generateStatus);
                }
            }
        });
    }
    /**
     * Returns the external reference to the native LLM implementation.
     *
     * @returns External reference object
     * @internal
     */
    getExternal() {
        return this.llm;
    }
    /**
     * Cleans up resources associated with this LLM.
     */
    destroy() {
        if (this.llm) {
            expose_binary_1.LLMInterfaceFunctions.delete(this.llm);
            this.llm = null;
        }
        if (this.factory) {
            expose_binary_1.LLMFactoryFunctions.delete(this.factory);
            this.factory = null;
        }
    }
    /**
     * Builds the generate status object for text generation.
     *
     * @param props - Configuration for text generation
     * @returns Promise resolving to the generate status object
     * @private
     */
    buildGenerateStatus(props) {
        return __awaiter(this, void 0, void 0, function* () {
            let generateStatus;
            let textGenerationConfig = null;
            try {
                textGenerationConfig = new text_generator_1.TextGenerationConfig(props.config);
                if (props.prompt) {
                    generateStatus = yield expose_binary_1.LLMInterfaceFunctions.generateTextFromPrompt(this.llm, props.prompt, textGenerationConfig.getExternal());
                }
                else {
                    generateStatus = yield expose_binary_1.LLMInterfaceFunctions.generateTextFromMessages(this.llm, props.messages.getExternal(), textGenerationConfig.getExternal());
                }
            }
            finally {
                textGenerationConfig === null || textGenerationConfig === void 0 ? void 0 : textGenerationConfig.destroy();
            }
            return generateStatus;
        });
    }
    /**
     * Creates a stream iterator for the generated text.
     *
     * @param inputStream - External reference to the input stream
     * @param messages - Optional messages object
     * @returns Stream iterator object
     * @private
     */
    streamIterator(inputStream, messages) {
        const stringStream = new stream_1.TextStream(inputStream, () => {
            expose_binary_1.InputStreamFunctions.deleteStream(inputStream);
            messages === null || messages === void 0 ? void 0 : messages.destroy();
        });
        return {
            getStream() {
                return stringStream.get();
            },
            next() {
                return __awaiter(this, void 0, void 0, function* () {
                    return stringStream.next();
                });
            },
        };
    }
}
exports.LLM = LLM;
